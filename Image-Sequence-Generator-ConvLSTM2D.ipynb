{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPB3-6q3SjkP"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.layers import ConvLSTM2D\n",
        "import numpy as np\n",
        "#import pylab as plt\n",
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive folder\n",
        "from google.colab import drive\n",
        "\n",
        "base_path = '/content/drive/My Drive/Tese/'\n",
        "if not (drive):\n",
        "  drive.mount(base_path)"
      ],
      "metadata": {
        "id": "eqdZmO9LUnEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import image\n",
        "tile = cv.imread(base_path + 'apple_grid_256x256.png')\n",
        "\n",
        "#Normal cv.imshow() function crashes Colab\n",
        "#from google.colab.patches import cv2_imshow\n",
        "#cv2_imshow(tile)\n",
        "\n",
        "#Display full tile\n",
        "plt.imshow(cv.cvtColor(tile, cv.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "#Normalize tile\n",
        "tile = np.array(tile, dtype=np.float32)\n",
        "tile = tile / 255\n",
        "\n",
        "#Display normalized tile (should look the same)\n",
        "plt.imshow(cv.cvtColor(tile, cv.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "#tile.shape #height, width, channels\n",
        "print('Image height:', tile.shape[0])\n",
        "print('Image width:', tile.shape[1])\n",
        "print('Color Channels:', tile.shape[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "TlF3vZkRSl54",
        "outputId": "aa455aa2-e88d-432a-fb4e-f47cd981924c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image height: 256\n",
            "Image width: 256\n",
            "Color Channels: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#User defined variables\n",
        "\n",
        "SEQUENCE = 10 #data points p/ target. NOT THE SAME AS BATCH SIZE\n",
        "\n",
        "grayscale_mode = True\n",
        "\n",
        "if grayscale_mode and tile.shape[2]==3:\n",
        "  tile = cv.cvtColor(tile, cv.COLOR_BGR2GRAY)\n",
        "  # Add a channel dimension since the images are grayscale.\n",
        "  tile = np.expand_dims(tile, axis=-1)\n",
        "  plt.imshow(tile[::,::, 0], cmap='gray', vmin=0, vmax=1)\n",
        "else:\n",
        "  plt.imshow(cv.cvtColor(tile, cv.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "print(tile.shape)\n",
        "roll_array = tile\n",
        "\n",
        "if grayscale_mode:\n",
        "  COLOR_CHANNELS = 1\n",
        "else:\n",
        "  COLOR_CHANNELS = 3\n",
        "\n",
        "#capture\n",
        "CAPTURE_WIDTH = 75\n",
        "CAPTURE_HEIGHT = 125\n",
        "\n",
        "#Roller\n",
        "ROLLER_SPEED = 7 #pixels per timestep\n",
        "if ROLLER_SPEED*SEQUENCE < CAPTURE_HEIGHT:\n",
        "  print(\"Warning: Area covered by input sequence smaller than pattern. Network may be unable to learn\")\n",
        "\n",
        "ROLLER_WIDTH = 300\n",
        "ROLLER_HEIGHT = 2000\n",
        "\n",
        "\n",
        "\n",
        "#Training\n",
        "NB_EPOCH = 2000\n",
        "BATCH_SIZE = 4 #tentar com batch size menor\n",
        "VALIDATION_SPLIT = 0.3\n",
        "VERBOSE = 1\n",
        "N_SAMPLES = 1000\n",
        "N_TRAIN_SAMPLES = int(N_SAMPLES * (1 - VALIDATION_SPLIT))\n",
        "N_VAL_SAMPLES = int(N_SAMPLES * VALIDATION_SPLIT)\n",
        "N_TRAINING_BATCHES = N_SAMPLES / BATCH_SIZE"
      ],
      "metadata": {
        "id": "KQou8uC_SqOQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "bd4b8b82-3240-45f1-ab5c-8c21f359c182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 1)\n",
            "Warning: Area covered by input sequence smaller than pattern. Network may be unable to learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def roller_frame_generator(starting_point):\n",
        "    \"\"\"Yields sequential single frames\"\"\"\n",
        "\n",
        "    roller_width = ROLLER_WIDTH\n",
        "    roller_height = ROLLER_HEIGHT\n",
        "    capture_height = CAPTURE_HEIGHT\n",
        "\n",
        "    #print(\"Tile shape:\", tile.shape)\n",
        "    roll_array = tile\n",
        "\n",
        "    horizontal_slice = tile\n",
        "\n",
        "    while horizontal_slice.shape[1] < roller_width:\n",
        "        horizontal_slice = np.concatenate((horizontal_slice, tile), axis=1)  # ---tile horizontally if necessary\n",
        "        #print(\"tilling horizontally. New roller shape:\", horizontal_slice.shape)\n",
        "\n",
        "    roll_array = horizontal_slice\n",
        "    #print(roll_array.shape)\n",
        "\n",
        "    #Set initial point for roller\n",
        "    roll_array = roll_array[starting_point:]\n",
        "\n",
        "    while True:\n",
        "\n",
        "        while roll_array.shape[0] < roller_height + capture_height:\n",
        "            roll_array = np.concatenate((roll_array, horizontal_slice), axis=0)  # ---tile vertically if necessary\n",
        "            #print(\"tilling verticaly. New roller shape:\", roll_array.shape)\n",
        "\n",
        "        #frame = np.zeros([CAPTURE_HEIGHT, CAPTURE_WIDTH, 3], dtype=np.uint8)\n",
        "        #frame = roll_array[0:CAPTURE_HEIGHT, 0:CAPTURE_WIDTH, ::]\n",
        "        yield roll_array[0:CAPTURE_HEIGHT, 0:CAPTURE_WIDTH].copy()\n",
        "        roll_array = roll_array[ROLLER_SPEED:]\n"
      ],
      "metadata": {
        "id": "atHGxtifSsUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequence generator calls the frame generator with a random starting point and yields a sequence of 10 frames, as well as the next frame, which is to be predicted.\n",
        "((10, capture_height, capture_width, channels), (capture_height, capture_width, channels))"
      ],
      "metadata": {
        "id": "da3sEV6BZiil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_generator():\n",
        "    sequence_size = 10\n",
        "    \"\"\"Yields sequences of frames\n",
        "    \"\"\"\n",
        "    #Call frame generator with random starting point\n",
        "    frame_gen = roller_frame_generator(random.randrange(0, tile.shape[0]))\n",
        "\n",
        "    #Create object to store the sequence of frames\n",
        "    #frame_sequence = np.zeros([sequence_size, CAPTURE_HEIGHT, CAPTURE_WIDTH, COLOR_CHANNELS], dtype=np.float32)\n",
        "    frame_sequence = next(frame_gen)\n",
        "    frame_sequence = np.expand_dims(frame_sequence, axis=0)\n",
        "\n",
        "    #Loop the generator for frames\n",
        "    for i in range(1, sequence_size):\n",
        "        #frame_sequence[i] = next(frame_gen)\n",
        "        frame_sequence = np.concatenate((frame_sequence, np.expand_dims(next(frame_gen), axis=0)), axis=0)\n",
        "        #plt.imshow(cv.cvtColor(frame_sequence[i], cv.COLOR_BGR2RGB))\n",
        "        #plt.show()\n",
        "    target_frame = next(frame_gen)\n",
        "    yield frame_sequence, target_frame.astype(np.float32)\n",
        "\n",
        "#Concatenate images for side-by-side viewing\n",
        "sequence_gen = sequence_generator()\n",
        "sequence = next(sequence_gen)\n",
        "sidebyside = sequence[0][0] #set = to first image\n",
        "for j in range(1, len(sequence[0])):\n",
        "    sidebyside = np.concatenate((sidebyside, sequence[0][j]), axis=1)\n",
        "plt.imshow(cv.cvtColor(sidebyside, cv.COLOR_BGR2RGB))\n",
        "plt.title(\"Input Sequence\")\n",
        "plt.show()\n",
        "\n",
        "#Plot Target Frame\n",
        "plt.imshow(cv.cvtColor(sequence[1], cv.COLOR_BGR2RGB))\n",
        "plt.title(\"Output Image\")\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "\n",
        "# Adds a subplot at the 1st position\n",
        "fig.add_subplot(1, 2, 1)\n",
        "\n",
        "# showing image\n",
        "plt.imshow(cv.cvtColor(sequence[0][-1], cv.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.title(\"Last Image in sequence\")\n",
        "\n",
        "# Adds a subplot at the 2nd position\n",
        "fig.add_subplot(1, 2, 2)\n",
        "\n",
        "# showing image\n",
        "plt.imshow(cv.cvtColor(sequence[1], cv.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.title(\"Target\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "EYYlPFONSuCA",
        "outputId": "97a9d6e4-d6fa-416e-f817-0df4cb13a0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x216 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to tf.Dataset object\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    sequence_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(SEQUENCE, CAPTURE_HEIGHT, CAPTURE_WIDTH, COLOR_CHANNELS), dtype=np.float32),\n",
        "        tf.TensorSpec(shape=(CAPTURE_HEIGHT, CAPTURE_WIDTH, COLOR_CHANNELS), dtype=np.float32)))\n",
        "\n",
        "input = dataset.batch(BATCH_SIZE)\n",
        "validation_data = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "tf.data.experimental.enable_debug_mode()"
      ],
      "metadata": {
        "id": "KQqcmTpUXqM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CONVLSTM2D:\n",
        "  @staticmethod\n",
        "  def build(batch_size, time_steps, rows, cols, channels):\n",
        "      inputNet = keras.Input(shape=(time_steps, rows, cols, channels)) #batch_shape=(10, 150, 100, 3)\n",
        "\n",
        "      #convlstm2d = layers.Rescaling(1./255)(inputNet)\n",
        "      #convlstm2d = layers.BatchNormalization()(inputNet)\n",
        "      convlstm2d = layers.ConvLSTM2D(64, kernel_size=(3,3), padding=\"same\", dropout=0.2, recurrent_dropout=0.1, return_sequences=True)(inputNet)\n",
        "      convlstm2d = layers.BatchNormalization()(convlstm2d)\n",
        "      convlstm2d = layers.ConvLSTM2D(64, kernel_size=(3,3), padding=\"same\", dropout=0.2, recurrent_dropout=0.1, return_sequences=True)(convlstm2d)\n",
        "      convlstm2d = layers.BatchNormalization()(convlstm2d)\n",
        "      #convlstm2d = layers.ConvLSTM2D(16, kernel_size=(3,3), padding=\"same\", dropout=0.2, recurrent_dropout=0.1, return_sequences=True)(convlstm2d)\n",
        "      #convlstm2d = layers.BatchNormalization()(convlstm2d)\n",
        "      convlstm2d = layers.ConvLSTM2D(channels, kernel_size=(3,3), padding=\"same\", dropout=0.2, recurrent_dropout=0.1, return_sequences=False)(convlstm2d)\n",
        "\n",
        "\n",
        "      model = keras.Model(inputNet, convlstm2d)\n",
        "      return model"
      ],
      "metadata": {
        "id": "rS6S5Yp1dxN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPTIMIZER = tf.keras.optimizers.SGD()#SGD()\n",
        "\n",
        "model = CONVLSTM2D.build(batch_size = BATCH_SIZE, time_steps=SEQUENCE, rows=CAPTURE_HEIGHT, cols=CAPTURE_WIDTH, channels=COLOR_CHANNELS)\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=OPTIMIZER, metrics=[\"mse\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "#tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
        "#esCallBack = EarlyStopping(monitor='val_acc', min_delta=0, patience=12, verbose=0, mode='max')\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.01,patience=5, min_lr=0.0001)\n",
        "best_checkpoint = keras.callbacks.ModelCheckpoint(base_path + 'models/ConvLSTM2D/ConvLSTM2D.h5', monitor='val_loss', save_best_only=True, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzn-QGNxd1qH",
        "outputId": "20b7dc54-52da-4c99-b4a8-17f81d1816a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 10, 125, 75, 1)]  0         \n",
            "                                                                 \n",
            " conv_lstm2d_3 (ConvLSTM2D)  (None, 10, 125, 75, 16)   9856      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 10, 125, 75, 16)  64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv_lstm2d_4 (ConvLSTM2D)  (None, 10, 125, 75, 16)   18496     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 10, 125, 75, 16)  64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv_lstm2d_5 (ConvLSTM2D)  (None, 125, 75, 1)        616       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,096\n",
            "Trainable params: 29,032\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = tf.keras.models.load_model('/content/drive/My Drive/Tese/ConvLSTM2D.h5')\n",
        "tf.config.run_functions_eagerly(True)\n",
        "history = model.fit(x = input,\n",
        "\t\tepochs=NB_EPOCH,\n",
        "\t\tverbose=1, # 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
        "\t\tvalidation_data=validation_data,\n",
        "\t\tcallbacks=[reduce_lr,best_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_DourA2d4wX",
        "outputId": "69cff4d6-e5ea-4272-fef2-b8aad9317c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 751/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 751: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 752/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 752: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 753/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 753: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 754/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 754: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 755/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0351 - mse: 0.0351\n",
            "Epoch 755: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 756/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 756: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 757/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 757: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 922ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 758/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 758: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 913ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 759/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 759: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 906ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 760/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0351 - mse: 0.0351\n",
            "Epoch 760: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 761/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0436 - mse: 0.0436\n",
            "Epoch 761: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 762/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 762: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 763/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 763: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 764/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 764: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 765/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 765: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0436 - val_mse: 0.0436 - lr: 1.0000e-04\n",
            "Epoch 766/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 766: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 986ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 767/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0339 - mse: 0.0339\n",
            "Epoch 767: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0404 - val_mse: 0.0404 - lr: 1.0000e-04\n",
            "Epoch 768/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 768: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 769/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 769: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 770/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 770: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0334 - val_mse: 0.0334 - lr: 1.0000e-04\n",
            "Epoch 771/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 771: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 772/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 772: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 773/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0439 - mse: 0.0439\n",
            "Epoch 773: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 774/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 774: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 775/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 775: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 776/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 776: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 777/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0339 - mse: 0.0339\n",
            "Epoch 777: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 778/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 778: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 779/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 779: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 780/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 780: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 839ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 781/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 781: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 782/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 782: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 783/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 783: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0334 - val_mse: 0.0334 - lr: 1.0000e-04\n",
            "Epoch 784/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 784: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 785/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 785: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 786/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 786: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 841ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 787/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 787: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 788/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 788: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 848ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 789/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 789: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 938ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 790/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 790: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 791/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 791: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 792/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0458 - mse: 0.0458\n",
            "Epoch 792: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.0458 - mse: 0.0458 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 793/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 793: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 794/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 794: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 795/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 795: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 796/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 796: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 797/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 797: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 798/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 798: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 799/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 799: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 800/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 800: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 801/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
            "Epoch 801: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 802/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 802: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0447 - val_mse: 0.0447 - lr: 1.0000e-04\n",
            "Epoch 803/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 803: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 804/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 804: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 805/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mse: 0.0408\n",
            "Epoch 805: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 806/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 806: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 807/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 807: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0335 - val_mse: 0.0335 - lr: 1.0000e-04\n",
            "Epoch 808/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 808: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 809/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 809: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 830ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 810/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 810: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 811/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 811: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 812/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0436 - mse: 0.0436\n",
            "Epoch 812: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 813/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 813: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 814/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 814: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 815/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 815: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 816/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 816: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 817/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 817: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 818/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 818: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 819/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 819: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 974ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0437 - val_mse: 0.0437 - lr: 1.0000e-04\n",
            "Epoch 820/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 820: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 821/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 821: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 822/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 822: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0437 - val_mse: 0.0437 - lr: 1.0000e-04\n",
            "Epoch 823/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 823: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0462 - val_mse: 0.0462 - lr: 1.0000e-04\n",
            "Epoch 824/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 824: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 825/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 825: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 835ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 826/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 826: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 827/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 827: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 828/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 828: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 829/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 829: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 830/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0457 - mse: 0.0457\n",
            "Epoch 830: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 842ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 831/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 831: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 832/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 832: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 833/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 833: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 834/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 834: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 835/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 835: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0462 - val_mse: 0.0462 - lr: 1.0000e-04\n",
            "Epoch 836/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 836: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 990ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 837/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 837: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 838/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 838: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 839/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 839: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 840/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 840: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 841/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 841: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 842/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 842: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 843/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 843: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0348 - val_mse: 0.0348 - lr: 1.0000e-04\n",
            "Epoch 844/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 844: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0436 - val_mse: 0.0436 - lr: 1.0000e-04\n",
            "Epoch 845/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 845: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 846/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 846: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 831ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 847/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 847: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 848/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 848: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 849/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 849: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 850/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 850: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 973ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 851/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 851: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 916ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 852/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 852: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 853/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 853: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 854/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 854: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 855/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 855: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 856/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 856: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 857/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 857: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0335 - val_mse: 0.0335 - lr: 1.0000e-04\n",
            "Epoch 858/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 858: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 859/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 859: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 860/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 860: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 861/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 861: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 862/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 862: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 863/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 863: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 864/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 864: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 865/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 865: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 842ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 866/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 866: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 867/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 867: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0348 - val_mse: 0.0348 - lr: 1.0000e-04\n",
            "Epoch 868/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 868: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 838ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 869/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 869: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 870/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 870: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 871/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 871: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 845ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0437 - val_mse: 0.0437 - lr: 1.0000e-04\n",
            "Epoch 872/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 872: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 873/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 873: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 874/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 874: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 875/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 875: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 876/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 876: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 877/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 877: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 878/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 878: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 879/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 879: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 880/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 880: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 881/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 881: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 882/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 882: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 883/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 883: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 964ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 884/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 884: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 885/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 885: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 832ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 886/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 886: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 887/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 887: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 888/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 888: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 913ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 889/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 889: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 890/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 890: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 891/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 891: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 892/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0457 - mse: 0.0457\n",
            "Epoch 892: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0457 - mse: 0.0457 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 893/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 893: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 894/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 894: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 900ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0348 - val_mse: 0.0348 - lr: 1.0000e-04\n",
            "Epoch 895/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
            "Epoch 895: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 896/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 896: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 897/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 897: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 912ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 898/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 898: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 899/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 899: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 900/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 900: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 901/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 901: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 902/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 902: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 903/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 903: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 904/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 904: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 905/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 905: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 920ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0335 - val_mse: 0.0335 - lr: 1.0000e-04\n",
            "Epoch 906/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 906: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 907/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 907: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 908/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 908: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 909/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 909: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 910/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 910: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 911/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 911: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 912/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 912: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 913/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mse: 0.0344\n",
            "Epoch 913: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 914/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 914: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 959ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 915/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 915: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 916/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 916: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0335 - val_mse: 0.0335 - lr: 1.0000e-04\n",
            "Epoch 917/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 917: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 918/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 918: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0335 - val_mse: 0.0335 - lr: 1.0000e-04\n",
            "Epoch 919/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 919: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 920/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 920: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 921/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 921: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 922/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 922: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 923/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0442 - mse: 0.0442\n",
            "Epoch 923: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 924/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 924: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 846ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0403 - val_mse: 0.0403 - lr: 1.0000e-04\n",
            "Epoch 925/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 925: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0436 - val_mse: 0.0436 - lr: 1.0000e-04\n",
            "Epoch 926/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 926: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 927/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 927: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 928/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 928: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 929/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 929: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 839ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 930/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 930: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 931/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 931: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 932/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0355 - mse: 0.0355\n",
            "Epoch 932: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 933/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 933: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 934/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 934: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 935/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 935: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 936/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 936: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 937/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 937: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 938/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 938: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 939/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 939: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 834ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 940/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mse: 0.0344\n",
            "Epoch 940: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 941/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 941: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 942/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 942: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 943/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 943: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 908ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 944/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 944: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 945/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 945: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 946/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 946: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 947/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 947: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 948/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 948: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 898ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0364 - val_mse: 0.0364 - lr: 1.0000e-04\n",
            "Epoch 949/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mse: 0.0344\n",
            "Epoch 949: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 846ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 950/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 950: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0334 - val_mse: 0.0334 - lr: 1.0000e-04\n",
            "Epoch 951/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0456 - mse: 0.0456\n",
            "Epoch 951: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0456 - mse: 0.0456 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 952/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 952: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 953/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0349 - mse: 0.0349\n",
            "Epoch 953: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 954/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 954: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 955/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 955: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 956/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mse: 0.0344\n",
            "Epoch 956: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 957/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 957: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 958/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 958: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0461 - val_mse: 0.0461 - lr: 1.0000e-04\n",
            "Epoch 959/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 959: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 960/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 960: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0334 - val_mse: 0.0334 - lr: 1.0000e-04\n",
            "Epoch 961/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 961: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0437 - val_mse: 0.0437 - lr: 1.0000e-04\n",
            "Epoch 962/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 962: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 963/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 963: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 964/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 964: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 965/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 965: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 966/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 966: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 967/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0455 - mse: 0.0455\n",
            "Epoch 967: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 968/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 968: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0353 - val_mse: 0.0353 - lr: 1.0000e-04\n",
            "Epoch 969/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 969: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 970/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 970: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0353 - val_mse: 0.0353 - lr: 1.0000e-04\n",
            "Epoch 971/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 971: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 991ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 972/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 972: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 973/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 973: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 974/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 974: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 975/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 975: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0416 - val_mse: 0.0416 - lr: 1.0000e-04\n",
            "Epoch 976/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 976: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 977/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 977: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 978/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 978: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 979/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 979: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 980/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0456 - mse: 0.0456\n",
            "Epoch 980: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0456 - mse: 0.0456 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 981/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0349 - mse: 0.0349\n",
            "Epoch 981: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 982/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 982: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 983/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 983: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 984/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 984: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0459 - val_mse: 0.0459 - lr: 1.0000e-04\n",
            "Epoch 985/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 985: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0334 - val_mse: 0.0334 - lr: 1.0000e-04\n",
            "Epoch 986/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 986: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 987/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0441 - mse: 0.0441\n",
            "Epoch 987: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0441 - mse: 0.0441 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 988/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 988: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 957ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 989/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0436 - mse: 0.0436\n",
            "Epoch 989: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.0348 - val_mse: 0.0348 - lr: 1.0000e-04\n",
            "Epoch 990/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 990: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 991/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 991: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 992/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 992: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 993/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 993: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0348 - val_mse: 0.0348 - lr: 1.0000e-04\n",
            "Epoch 994/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 994: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 995/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 995: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0334 - val_mse: 0.0334 - lr: 1.0000e-04\n",
            "Epoch 996/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 996: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 997/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0455 - mse: 0.0455\n",
            "Epoch 997: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 998/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0436 - mse: 0.0436\n",
            "Epoch 998: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 999/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 999: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1000/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0336 - mse: 0.0336\n",
            "Epoch 1000: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1001/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 1001: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1002/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1002: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1003/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1003: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1004/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1004: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1005/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 1005: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1006/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1006: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 842ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1007/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0436 - mse: 0.0436\n",
            "Epoch 1007: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 1008/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1008: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 917ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1009/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1009: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 930ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1010/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1010: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 915ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1011/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1011: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1012/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1012: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1013/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1013: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0416 - val_mse: 0.0416 - lr: 1.0000e-04\n",
            "Epoch 1014/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0455 - mse: 0.0455\n",
            "Epoch 1014: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0455 - mse: 0.0455 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 1015/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1015: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1016/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1016: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1017/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1017: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0459 - val_mse: 0.0459 - lr: 1.0000e-04\n",
            "Epoch 1018/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 1018: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1019/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1019: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0407 - val_mse: 0.0407 - lr: 1.0000e-04\n",
            "Epoch 1020/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1020: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1021/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1021: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 897ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1022/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0348 - mse: 0.0348\n",
            "Epoch 1022: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.0415 - val_mse: 0.0415 - lr: 1.0000e-04\n",
            "Epoch 1023/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1023: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0445 - val_mse: 0.0445 - lr: 1.0000e-04\n",
            "Epoch 1024/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1024: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1025/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1025: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0374 - val_mse: 0.0374 - lr: 1.0000e-04\n",
            "Epoch 1026/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1026: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 1027/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0336 - mse: 0.0336\n",
            "Epoch 1027: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1028/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1028: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1029/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0348 - mse: 0.0348\n",
            "Epoch 1029: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 957ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1030/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 1030: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1031/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1031: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1032/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1032: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1033/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0436 - mse: 0.0436\n",
            "Epoch 1033: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 826ms/step - loss: 0.0436 - mse: 0.0436 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1034/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 1034: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1035/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1035: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0440 - val_mse: 0.0440 - lr: 1.0000e-04\n",
            "Epoch 1036/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1036: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 846ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1037/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1037: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1038/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1038: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1039/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1039: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1040/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1040: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0334 - val_mse: 0.0334 - lr: 1.0000e-04\n",
            "Epoch 1041/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0348 - mse: 0.0348\n",
            "Epoch 1041: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.0436 - val_mse: 0.0436 - lr: 1.0000e-04\n",
            "Epoch 1042/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1042: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1043/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1043: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 961ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0418 - val_mse: 0.0418 - lr: 1.0000e-04\n",
            "Epoch 1044/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1044: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 1045/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1045: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1046/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1046: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1047/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1047: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1048/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1048: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0440 - val_mse: 0.0440 - lr: 1.0000e-04\n",
            "Epoch 1049/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1049: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0340 - val_mse: 0.0340 - lr: 1.0000e-04\n",
            "Epoch 1050/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1050: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1051/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 1051: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 1052/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1052: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0445 - val_mse: 0.0445 - lr: 1.0000e-04\n",
            "Epoch 1053/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1053: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1054/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1054: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1055/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1055: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1056/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1056: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1057/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1057: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 929ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0459 - val_mse: 0.0459 - lr: 1.0000e-04\n",
            "Epoch 1058/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1058: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1059/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 1059: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1060/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1060: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1061/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1061: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 843ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1062/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1062: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1063/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 1063: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1064/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 1064: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0418 - val_mse: 0.0418 - lr: 1.0000e-04\n",
            "Epoch 1065/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1065: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0458 - val_mse: 0.0458 - lr: 1.0000e-04\n",
            "Epoch 1066/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1066: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 917ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1067/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1067: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1068/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1068: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1069/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1069: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1070/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1070: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0458 - val_mse: 0.0458 - lr: 1.0000e-04\n",
            "Epoch 1071/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1071: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1072/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1072: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1073/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1073: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1074/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1074: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1075/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1075: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1076/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1076: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1077/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1077: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1078/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1078: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 1079/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1079: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1080/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1080: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 909ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1081/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1081: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1082/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1082: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0459 - val_mse: 0.0459 - lr: 1.0000e-04\n",
            "Epoch 1083/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1083: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1084/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1084: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1085/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 1085: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1086/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1086: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1087/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1087: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 903ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1088/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1088: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1089/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1089: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1090/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1090: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 941ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0340 - val_mse: 0.0340 - lr: 1.0000e-04\n",
            "Epoch 1091/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1091: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1092/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0439 - mse: 0.0439\n",
            "Epoch 1092: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0439 - mse: 0.0439 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1093/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 1093: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1094/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 1094: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1095/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1095: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1096/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1096: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 989ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1097/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1097: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1098/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 1098: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.0399 - val_mse: 0.0399 - lr: 1.0000e-04\n",
            "Epoch 1099/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1099: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1100/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1100: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1101/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1101: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1102/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0354 - mse: 0.0354\n",
            "Epoch 1102: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1103/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1103: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1104/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0453 - mse: 0.0453\n",
            "Epoch 1104: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 957ms/step - loss: 0.0453 - mse: 0.0453 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1105/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1105: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 916ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1106/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 1106: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1107/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1107: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1108/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1108: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1109/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1109: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 846ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0414 - val_mse: 0.0414 - lr: 1.0000e-04\n",
            "Epoch 1110/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1110: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1111/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 1111: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1112/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1112: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0399 - val_mse: 0.0399 - lr: 1.0000e-04\n",
            "Epoch 1113/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0347 - mse: 0.0347\n",
            "Epoch 1113: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 927ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.0458 - val_mse: 0.0458 - lr: 1.0000e-04\n",
            "Epoch 1114/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1114: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 1115/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1115: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1116/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1116: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1117/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1117: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1118/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 1118: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.0436 - val_mse: 0.0436 - lr: 1.0000e-04\n",
            "Epoch 1119/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 1119: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1120/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0353 - mse: 0.0353\n",
            "Epoch 1120: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 846ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1121/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 1121: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1122/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1122: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1123/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 1123: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0352 - val_mse: 0.0352 - lr: 1.0000e-04\n",
            "Epoch 1124/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1124: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1125/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1125: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1126/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1126: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1127/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1127: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 921ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1128/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1128: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1129/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 1129: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.0458 - val_mse: 0.0458 - lr: 1.0000e-04\n",
            "Epoch 1130/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1130: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1131/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1131: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1132/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1132: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1133/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 1133: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1134/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 1134: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1135/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1135: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 909ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1136/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 1136: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 977ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1137/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1137: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 937ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1138/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1138: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1139/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1139: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1140/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1140: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 1141/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1141: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1142/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 1142: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1143/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0353 - mse: 0.0353\n",
            "Epoch 1143: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1144/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1144: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1145/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 1145: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1146/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 1146: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0457 - val_mse: 0.0457 - lr: 1.0000e-04\n",
            "Epoch 1147/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mse: 0.0408\n",
            "Epoch 1147: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 898ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1148/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1148: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1149/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0453 - mse: 0.0453\n",
            "Epoch 1149: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 0.0453 - mse: 0.0453 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1150/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 1150: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1151/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1151: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 997ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1152/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1152: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1153/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1153: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1154/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1154: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1155/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0353 - mse: 0.0353\n",
            "Epoch 1155: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1156/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1156: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1157/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1157: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0438 - val_mse: 0.0438 - lr: 1.0000e-04\n",
            "Epoch 1158/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 1158: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 841ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1159/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 1159: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1160/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1160: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1161/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1161: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 1162/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 1162: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1163/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 1163: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1164/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 1164: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1165/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1165: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1166/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1166: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1167/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1167: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1168/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1168: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 962ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1169/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 1169: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1170/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1170: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 943ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1171/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1171: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1172/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 1172: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1173/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 1173: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1174/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0453 - mse: 0.0453\n",
            "Epoch 1174: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0453 - mse: 0.0453 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 1175/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1175: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1176/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0438 - mse: 0.0438\n",
            "Epoch 1176: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0438 - mse: 0.0438 - val_loss: 0.0346 - val_mse: 0.0346 - lr: 1.0000e-04\n",
            "Epoch 1177/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1177: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1178/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0434 - mse: 0.0434\n",
            "Epoch 1178: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.0414 - val_mse: 0.0414 - lr: 1.0000e-04\n",
            "Epoch 1179/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1179: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1180/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1180: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1181/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1181: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1182/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1182: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 1183/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 1183: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1184/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1184: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1185/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1185: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 837ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0332 - val_mse: 0.0332 - lr: 1.0000e-04\n",
            "Epoch 1186/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 1186: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1187/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1187: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1188/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1188: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1189/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1189: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0332 - val_mse: 0.0332 - lr: 1.0000e-04\n",
            "Epoch 1190/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1190: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1191/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1191: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1192/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1192: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1193/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1193: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 1194/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1194: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1195/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1195: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 1196/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1196: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1197/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1197: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1198/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1198: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 962ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 1199/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1199: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1200/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1200: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1201/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 1201: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1202/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1202: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1203/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1203: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1204/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1204: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0416 - val_mse: 0.0416 - lr: 1.0000e-04\n",
            "Epoch 1205/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 1205: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1206/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1206: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1207/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1207: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0457 - val_mse: 0.0457 - lr: 1.0000e-04\n",
            "Epoch 1208/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1208: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1209/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 1209: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1210/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 1210: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 843ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 1211/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1211: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0457 - val_mse: 0.0457 - lr: 1.0000e-04\n",
            "Epoch 1212/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1212: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1213/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1213: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0416 - val_mse: 0.0416 - lr: 1.0000e-04\n",
            "Epoch 1214/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 1214: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1215/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 1215: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1216/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 1216: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0404 - val_mse: 0.0404 - lr: 1.0000e-04\n",
            "Epoch 1217/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1217: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1218/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1218: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1219/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0452 - mse: 0.0452\n",
            "Epoch 1219: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0332 - val_mse: 0.0332 - lr: 1.0000e-04\n",
            "Epoch 1220/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1220: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1221/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1221: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1222/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1222: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1223/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1223: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 1224/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1224: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1225/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1225: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1226/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1226: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1227/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1227: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1228/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0452 - mse: 0.0452\n",
            "Epoch 1228: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0332 - val_mse: 0.0332 - lr: 1.0000e-04\n",
            "Epoch 1229/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1229: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 907ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1230/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1230: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1231/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 1231: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 922ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1232/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 1232: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1233/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0452 - mse: 0.0452\n",
            "Epoch 1233: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 1234/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1234: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0364 - val_mse: 0.0364 - lr: 1.0000e-04\n",
            "Epoch 1235/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1235: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 1236/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0341 - mse: 0.0341\n",
            "Epoch 1236: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0341 - mse: 0.0341 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 1237/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1237: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1238/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 1238: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1239/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0452 - mse: 0.0452\n",
            "Epoch 1239: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1240/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1240: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1241/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1241: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1242/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1242: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1243/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 1243: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1244/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1244: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 914ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1245/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 1245: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 1246/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0452 - mse: 0.0452\n",
            "Epoch 1246: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0452 - mse: 0.0452 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 1247/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 1247: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1248/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1248: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1249/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1249: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1250/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1250: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1251/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1251: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0399 - val_mse: 0.0399 - lr: 1.0000e-04\n",
            "Epoch 1252/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1252: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1253/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1253: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1254/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1254: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1255/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1255: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1256/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1256: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1257/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1257: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 905ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1258/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
            "Epoch 1258: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1259/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1259: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1260/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1260: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 923ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0404 - val_mse: 0.0404 - lr: 1.0000e-04\n",
            "Epoch 1261/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 1261: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1262/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1262: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1263/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1263: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0399 - val_mse: 0.0399 - lr: 1.0000e-04\n",
            "Epoch 1264/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1264: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1265/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1265: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1266/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
            "Epoch 1266: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1267/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
            "Epoch 1267: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1268/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1268: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 1269/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1269: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0456 - val_mse: 0.0456 - lr: 1.0000e-04\n",
            "Epoch 1270/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1270: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 905ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1271/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1271: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1272/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1272: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 898ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1273/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1273: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1274/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1274: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1275/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
            "Epoch 1275: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1276/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1276: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1277/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414\n",
            "Epoch 1277: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0414 - mse: 0.0414 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1278/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 1278: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1279/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 1279: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1280/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1280: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1281/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 1281: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1282/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1282: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1283/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 1283: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1284/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1284: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1285/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1285: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 916ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0416 - val_mse: 0.0416 - lr: 1.0000e-04\n",
            "Epoch 1286/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0451 - mse: 0.0451\n",
            "Epoch 1286: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1287/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1287: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1288/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1288: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1289/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1289: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1290/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mse: 0.0408\n",
            "Epoch 1290: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1291/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1291: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1292/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1292: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 923ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1293/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1293: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 900ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1294/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1294: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1295/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1295: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1296/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1296: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1297/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 1297: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1298/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1298: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1299/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1299: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 906ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1300/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1300: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 912ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1301/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1301: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1302/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1302: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 976ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1303/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1303: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1304/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0451 - mse: 0.0451\n",
            "Epoch 1304: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1305/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 1305: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1306/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 1306: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1307/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1307: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 897ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1308/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1308: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1309/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1309: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1310/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 1310: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 1311/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1311: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1312/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0451 - mse: 0.0451\n",
            "Epoch 1312: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1313/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1313: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 900ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1314/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1314: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0441 - val_mse: 0.0441 - lr: 1.0000e-04\n",
            "Epoch 1315/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0451 - mse: 0.0451\n",
            "Epoch 1315: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1316/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 1316: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1317/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0340 - mse: 0.0340\n",
            "Epoch 1317: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1318/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1318: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1319/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 1319: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1000ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0407 - val_mse: 0.0407 - lr: 1.0000e-04\n",
            "Epoch 1320/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1320: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1321/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1321: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1322/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1322: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 925ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1323/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1323: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1324/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1324: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1325/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1325: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0455 - val_mse: 0.0455 - lr: 1.0000e-04\n",
            "Epoch 1326/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1326: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1327/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1327: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0415 - val_mse: 0.0415 - lr: 1.0000e-04\n",
            "Epoch 1328/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1328: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 900ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1329/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1329: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1330/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 1330: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1331/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1331: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1332/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0340 - mse: 0.0340\n",
            "Epoch 1332: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0340 - mse: 0.0340 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1333/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1333: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1334/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 1334: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0364 - val_mse: 0.0364 - lr: 1.0000e-04\n",
            "Epoch 1335/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1335: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1336/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1336: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 911ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0441 - val_mse: 0.0441 - lr: 1.0000e-04\n",
            "Epoch 1337/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 1337: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1338/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1338: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1339/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0451 - mse: 0.0451\n",
            "Epoch 1339: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1340/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1340: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1341/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1341: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1342/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1342: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 960ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0415 - val_mse: 0.0415 - lr: 1.0000e-04\n",
            "Epoch 1343/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1343: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1344/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1344: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1345/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1345: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1346/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1346: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1347/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1347: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0454 - val_mse: 0.0454 - lr: 1.0000e-04\n",
            "Epoch 1348/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0450 - mse: 0.0450\n",
            "Epoch 1348: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1349/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1349: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1350/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1350: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1351/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 1351: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0364 - val_mse: 0.0364 - lr: 1.0000e-04\n",
            "Epoch 1352/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1352: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 947ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1353/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1353: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 960ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1354/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1354: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 897ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1355/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0450 - mse: 0.0450\n",
            "Epoch 1355: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0345 - val_mse: 0.0345 - lr: 1.0000e-04\n",
            "Epoch 1356/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0333 - mse: 0.0333\n",
            "Epoch 1356: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0333 - mse: 0.0333 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1357/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1357: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 1358/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1358: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1359/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0345 - mse: 0.0345\n",
            "Epoch 1359: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 907ms/step - loss: 0.0345 - mse: 0.0345 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1360/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1360: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1361/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1361: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1362/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1362: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0349 - val_mse: 0.0349 - lr: 1.0000e-04\n",
            "Epoch 1363/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0450 - mse: 0.0450\n",
            "Epoch 1363: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1364/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1364: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1365/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1365: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 840ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1366/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1366: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1367/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1367: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 1368/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1368: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1369/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1369: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 1370/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 1370: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1371/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1371: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 1372/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1372: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1373/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0351 - mse: 0.0351\n",
            "Epoch 1373: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0351 - mse: 0.0351 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1374/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1374: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1375/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1375: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1376/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1376: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1377/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1377: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1378/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1378: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 960ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1379/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 1379: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1380/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1380: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1381/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1381: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1382/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 1382: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1383/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1383: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0440 - val_mse: 0.0440 - lr: 1.0000e-04\n",
            "Epoch 1384/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1384: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0440 - val_mse: 0.0440 - lr: 1.0000e-04\n",
            "Epoch 1385/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1385: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1386/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mse: 0.0413\n",
            "Epoch 1386: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0413 - mse: 0.0413 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1387/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1387: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1388/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1388: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1389/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1389: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1390/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 1390: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1391/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1391: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1392/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1392: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1393/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1393: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1394/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 1394: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1395/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 1395: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 905ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1396/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1396: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0414 - val_mse: 0.0414 - lr: 1.0000e-04\n",
            "Epoch 1397/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1397: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1398/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1398: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1399/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1399: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1400/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435\n",
            "Epoch 1400: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 953ms/step - loss: 0.0435 - mse: 0.0435 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1401/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1401: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1402/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1402: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1403/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 1403: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0344 - val_mse: 0.0344 - lr: 1.0000e-04\n",
            "Epoch 1404/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1404: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1405/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0450 - mse: 0.0450\n",
            "Epoch 1405: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0450 - mse: 0.0450 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1406/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1406: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0407 - val_mse: 0.0407 - lr: 1.0000e-04\n",
            "Epoch 1407/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1407: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 946ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0407 - val_mse: 0.0407 - lr: 1.0000e-04\n",
            "Epoch 1408/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1408: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1409/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1409: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1410/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1410: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 909ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1411/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1411: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1412/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 1412: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1413/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1413: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1414/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 1414: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 972ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0344 - val_mse: 0.0344 - lr: 1.0000e-04\n",
            "Epoch 1415/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1415: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 911ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1416/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1416: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0454 - val_mse: 0.0454 - lr: 1.0000e-04\n",
            "Epoch 1417/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1417: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1418/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 1418: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0404 - val_mse: 0.0404 - lr: 1.0000e-04\n",
            "Epoch 1419/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mse: 0.0408\n",
            "Epoch 1419: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 904ms/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1420/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1420: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1421/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1421: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1422/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1422: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1423/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1423: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1424/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1424: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 968ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1425/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 1425: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1426/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 1426: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0336 - val_mse: 0.0336 - lr: 1.0000e-04\n",
            "Epoch 1427/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1427: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1428/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1428: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1429/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1429: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 1430/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1430: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0453 - val_mse: 0.0453 - lr: 1.0000e-04\n",
            "Epoch 1431/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1431: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1432/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0449 - mse: 0.0449\n",
            "Epoch 1432: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 1433/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1433: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1434/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1434: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1435/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1435: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1436/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 1436: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 900ms/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1437/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1437: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0348 - val_mse: 0.0348 - lr: 1.0000e-04\n",
            "Epoch 1438/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1438: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1439/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1439: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0364 - val_mse: 0.0364 - lr: 1.0000e-04\n",
            "Epoch 1440/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 1440: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1441/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1441: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 956ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1442/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1442: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1443/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1443: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1444/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1444: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1445/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1445: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 913ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1446/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1446: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 958ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1447/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 1447: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 929ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1448/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1448: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 901ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1449/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 1449: val_loss did not improve from 0.03303\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1450/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1450: val_loss improved from 0.03303 to 0.03297, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0330 - val_mse: 0.0330 - lr: 1.0000e-04\n",
            "Epoch 1451/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0412 - mse: 0.0412\n",
            "Epoch 1451: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0412 - mse: 0.0412 - val_loss: 0.0414 - val_mse: 0.0414 - lr: 1.0000e-04\n",
            "Epoch 1452/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1452: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1453/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1453: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1454/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1454: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 930ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1455/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1455: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 927ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1456/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1456: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 933ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1457/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1457: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1458/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1458: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1459/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1459: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0439 - val_mse: 0.0439 - lr: 1.0000e-04\n",
            "Epoch 1460/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 1460: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1461/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0449 - mse: 0.0449\n",
            "Epoch 1461: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1462/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mse: 0.0332\n",
            "Epoch 1462: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1463/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1463: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1464/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1464: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1465/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1465: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0348 - val_mse: 0.0348 - lr: 1.0000e-04\n",
            "Epoch 1466/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1466: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1467/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1467: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 1468/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1468: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1469/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 1469: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1470/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1470: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0439 - val_mse: 0.0439 - lr: 1.0000e-04\n",
            "Epoch 1471/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1471: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 848ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0413 - val_mse: 0.0413 - lr: 1.0000e-04\n",
            "Epoch 1472/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1472: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1473/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1473: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1474/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1474: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1475/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0449 - mse: 0.0449\n",
            "Epoch 1475: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 901ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1476/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0350 - mse: 0.0350\n",
            "Epoch 1476: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0350 - mse: 0.0350 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1477/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 1477: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1478/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1478: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 996ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1479/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1479: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1480/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1480: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1481/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1481: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1482/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1482: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1483/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1483: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1484/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1484: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1485/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1485: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0336 - val_mse: 0.0336 - lr: 1.0000e-04\n",
            "Epoch 1486/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1486: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0439 - val_mse: 0.0439 - lr: 1.0000e-04\n",
            "Epoch 1487/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0449 - mse: 0.0449\n",
            "Epoch 1487: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1488/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1488: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1489/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1489: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1490/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1490: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1491/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 1491: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1492/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1492: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1493/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1493: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1494/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0411 - mse: 0.0411\n",
            "Epoch 1494: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0411 - mse: 0.0411 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1495/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1495: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 943ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1496/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mse: 0.0344\n",
            "Epoch 1496: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1497/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1497: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0374 - val_mse: 0.0374 - lr: 1.0000e-04\n",
            "Epoch 1498/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1498: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1499/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1499: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0374 - val_mse: 0.0374 - lr: 1.0000e-04\n",
            "Epoch 1500/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1500: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1501/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1501: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0453 - val_mse: 0.0453 - lr: 1.0000e-04\n",
            "Epoch 1502/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 1502: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1503/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0448 - mse: 0.0448\n",
            "Epoch 1503: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1504/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1504: val_loss did not improve from 0.03297\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1505/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1505: val_loss improved from 0.03297 to 0.03290, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0329 - val_mse: 0.0329 - lr: 1.0000e-04\n",
            "Epoch 1506/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 1506: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1507/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0363 - mse: 0.0363\n",
            "Epoch 1507: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1508/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1508: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1509/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1509: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1510/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1510: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1511/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1511: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1512/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 1512: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1513/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1513: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1514/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1514: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1515/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1515: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1516/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1516: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 898ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1517/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1517: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0336 - val_mse: 0.0336 - lr: 1.0000e-04\n",
            "Epoch 1518/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1518: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1519/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0349 - mse: 0.0349\n",
            "Epoch 1519: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1520/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1520: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1521/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1521: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1522/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1522: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 952ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1523/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1523: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1524/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1524: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1525/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 1525: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1526/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0349 - mse: 0.0349\n",
            "Epoch 1526: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 898ms/step - loss: 0.0349 - mse: 0.0349 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1527/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1527: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1528/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1528: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1529/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0448 - mse: 0.0448\n",
            "Epoch 1529: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0364 - val_mse: 0.0364 - lr: 1.0000e-04\n",
            "Epoch 1530/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1530: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1531/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 1531: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1532/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1532: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1533/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1533: val_loss did not improve from 0.03290\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1534/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1534: val_loss improved from 0.03290 to 0.03289, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0329 - val_mse: 0.0329 - lr: 1.0000e-04\n",
            "Epoch 1535/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1535: val_loss did not improve from 0.03289\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1536/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0394 - mse: 0.0394\n",
            "Epoch 1536: val_loss did not improve from 0.03289\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.0453 - val_mse: 0.0453 - lr: 1.0000e-04\n",
            "Epoch 1537/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mse: 0.0338\n",
            "Epoch 1537: val_loss did not improve from 0.03289\n",
            "1/1 [==============================] - 1s 932ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1538/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1538: val_loss did not improve from 0.03289\n",
            "1/1 [==============================] - 1s 920ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1539/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 1539: val_loss improved from 0.03289 to 0.03288, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 920ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0329 - val_mse: 0.0329 - lr: 1.0000e-04\n",
            "Epoch 1540/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1540: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1541/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1541: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1542/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0363 - mse: 0.0363\n",
            "Epoch 1542: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1543/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 1543: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 905ms/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1544/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1544: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1545/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1545: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1546/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 1546: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1547/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0331 - mse: 0.0331\n",
            "Epoch 1547: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0331 - mse: 0.0331 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1548/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 1548: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1549/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1549: val_loss did not improve from 0.03288\n",
            "1/1 [==============================] - 1s 992ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1550/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1550: val_loss improved from 0.03288 to 0.03285, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0329 - val_mse: 0.0329 - lr: 1.0000e-04\n",
            "Epoch 1551/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1551: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1552/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1552: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1553/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1553: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1554/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 1554: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 906ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1555/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1555: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1556/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0363 - mse: 0.0363\n",
            "Epoch 1556: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.0335 - val_mse: 0.0335 - lr: 1.0000e-04\n",
            "Epoch 1557/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1557: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1558/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1558: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1559/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 1559: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1560/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1560: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1561/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 1561: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1562/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1562: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 1563/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1563: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 1564/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1564: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1565/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 1565: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1566/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1566: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0335 - val_mse: 0.0335 - lr: 1.0000e-04\n",
            "Epoch 1567/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1567: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1568/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1568: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1569/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1569: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 948ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1570/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1570: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 926ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1571/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 1571: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1572/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 1572: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1573/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1573: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1574/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1574: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1575/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0448 - mse: 0.0448\n",
            "Epoch 1575: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0448 - mse: 0.0448 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1576/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 1576: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1577/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1577: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1578/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mse: 0.0429\n",
            "Epoch 1578: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0429 - mse: 0.0429 - val_loss: 0.0404 - val_mse: 0.0404 - lr: 1.0000e-04\n",
            "Epoch 1579/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1579: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1580/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 1580: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1581/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 1581: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1582/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 1582: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1583/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1583: val_loss did not improve from 0.03285\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1584/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1584: val_loss improved from 0.03285 to 0.03284, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0328 - val_mse: 0.0328 - lr: 1.0000e-04\n",
            "Epoch 1585/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1585: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0433 - val_mse: 0.0433 - lr: 1.0000e-04\n",
            "Epoch 1586/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1586: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 1587/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 1587: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1588/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1588: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0412 - val_mse: 0.0412 - lr: 1.0000e-04\n",
            "Epoch 1589/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 1589: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1590/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1590: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1591/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0433 - mse: 0.0433\n",
            "Epoch 1591: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1592/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1592: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1593/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 1593: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1594/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1594: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1595/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 1595: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 903ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1596/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mse: 0.0410\n",
            "Epoch 1596: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1597/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 1597: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1598/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1598: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1599/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mse: 0.0343\n",
            "Epoch 1599: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1600/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 1600: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 976ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 1601/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1601: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1602/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0447 - mse: 0.0447\n",
            "Epoch 1602: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0447 - mse: 0.0447 - val_loss: 0.0452 - val_mse: 0.0452 - lr: 1.0000e-04\n",
            "Epoch 1603/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1603: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1604/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 1604: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 903ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1605/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0363 - mse: 0.0363\n",
            "Epoch 1605: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1606/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1606: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1607/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1607: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1608/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 1608: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 922ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1609/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1609: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1610/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1610: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 846ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1611/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 1611: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1612/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1612: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0347 - val_mse: 0.0347 - lr: 1.0000e-04\n",
            "Epoch 1613/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1613: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 938ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1614/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1614: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1615/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1615: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1616/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1616: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1617/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1617: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1618/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1618: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1619/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1619: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 890ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1620/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 1620: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1621/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1621: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0404 - val_mse: 0.0404 - lr: 1.0000e-04\n",
            "Epoch 1622/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1622: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 962ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1623/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0342 - mse: 0.0342\n",
            "Epoch 1623: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 898ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0408 - val_mse: 0.0408 - lr: 1.0000e-04\n",
            "Epoch 1624/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1624: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1625/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1625: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1626/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1626: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1627/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1627: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1628/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1628: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1629/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mse: 0.0337\n",
            "Epoch 1629: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 1630/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 1630: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1631/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 1631: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0430 - val_mse: 0.0430 - lr: 1.0000e-04\n",
            "Epoch 1632/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 1632: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0424 - val_mse: 0.0424 - lr: 1.0000e-04\n",
            "Epoch 1633/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1633: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1634/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1634: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1635/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 1635: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1636/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1636: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0424 - val_mse: 0.0424 - lr: 1.0000e-04\n",
            "Epoch 1637/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393\n",
            "Epoch 1637: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1638/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1638: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1639/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1639: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1640/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1640: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1641/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1641: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1642/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 1642: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0346 - val_mse: 0.0346 - lr: 1.0000e-04\n",
            "Epoch 1643/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0432 - mse: 0.0432\n",
            "Epoch 1643: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1644/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 1644: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 924ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1645/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1645: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1646/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1646: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 1647/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 1647: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1648/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mse: 0.0405\n",
            "Epoch 1648: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1649/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0355 - mse: 0.0355\n",
            "Epoch 1649: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1650/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 1650: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0356 - val_mse: 0.0356 - lr: 1.0000e-04\n",
            "Epoch 1651/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0447 - mse: 0.0447\n",
            "Epoch 1651: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 888ms/step - loss: 0.0447 - mse: 0.0447 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1652/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 1652: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 822ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1653/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 1653: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1654/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1654: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1655/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1655: val_loss did not improve from 0.03284\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1656/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
            "Epoch 1656: val_loss improved from 0.03284 to 0.03276, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0328 - val_mse: 0.0328 - lr: 1.0000e-04\n",
            "Epoch 1657/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1657: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1658/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1658: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1659/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
            "Epoch 1659: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1660/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
            "Epoch 1660: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 820ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1661/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1661: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 953ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1662/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0330 - mse: 0.0330\n",
            "Epoch 1662: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 929ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1663/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1663: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1664/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1664: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1665/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1665: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0451 - val_mse: 0.0451 - lr: 1.0000e-04\n",
            "Epoch 1666/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
            "Epoch 1666: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0411 - val_mse: 0.0411 - lr: 1.0000e-04\n",
            "Epoch 1667/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1667: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 905ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0346 - val_mse: 0.0346 - lr: 1.0000e-04\n",
            "Epoch 1668/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0348 - mse: 0.0348\n",
            "Epoch 1668: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0348 - mse: 0.0348 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1669/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0379 - mse: 0.0379\n",
            "Epoch 1669: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1670/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1670: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0436 - val_mse: 0.0436 - lr: 1.0000e-04\n",
            "Epoch 1671/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1671: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1672/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1672: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0346 - val_mse: 0.0346 - lr: 1.0000e-04\n",
            "Epoch 1673/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 1673: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1674/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1674: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0399 - val_mse: 0.0399 - lr: 1.0000e-04\n",
            "Epoch 1675/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1675: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0403 - val_mse: 0.0403 - lr: 1.0000e-04\n",
            "Epoch 1676/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1676: val_loss did not improve from 0.03276\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0451 - val_mse: 0.0451 - lr: 1.0000e-04\n",
            "Epoch 1677/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
            "Epoch 1677: val_loss improved from 0.03276 to 0.03273, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0327 - val_mse: 0.0327 - lr: 1.0000e-04\n",
            "Epoch 1678/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0342 - mse: 0.0342\n",
            "Epoch 1678: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1679/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1679: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1680/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1680: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0450 - val_mse: 0.0450 - lr: 1.0000e-04\n",
            "Epoch 1681/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1681: val_loss improved from 0.03273 to 0.03273, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 911ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0327 - val_mse: 0.0327 - lr: 1.0000e-04\n",
            "Epoch 1682/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1682: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 933ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0407 - val_mse: 0.0407 - lr: 1.0000e-04\n",
            "Epoch 1683/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0342 - mse: 0.0342\n",
            "Epoch 1683: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0342 - mse: 0.0342 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1684/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1684: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1685/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1685: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1686/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1686: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0355 - val_mse: 0.0355 - lr: 1.0000e-04\n",
            "Epoch 1687/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1687: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0327 - val_mse: 0.0327 - lr: 1.0000e-04\n",
            "Epoch 1688/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 1688: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1689/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1689: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0432 - val_mse: 0.0432 - lr: 1.0000e-04\n",
            "Epoch 1690/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1690: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1691/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1691: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1692/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1692: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1693/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1693: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1694/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1694: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0419 - val_mse: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 1695/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1695: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 903ms/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1696/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1696: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0419 - val_mse: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 1697/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 1697: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1698/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 1698: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0450 - val_mse: 0.0450 - lr: 1.0000e-04\n",
            "Epoch 1699/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1699: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 943ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0419 - val_mse: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 1700/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1700: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1701/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1701: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0366 - val_mse: 0.0366 - lr: 1.0000e-04\n",
            "Epoch 1702/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 1702: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1703/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1703: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1704/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1704: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0419 - val_mse: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 1705/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1705: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1706/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1706: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1707/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1707: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0403 - val_mse: 0.0403 - lr: 1.0000e-04\n",
            "Epoch 1708/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 1708: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0431 - val_mse: 0.0431 - lr: 1.0000e-04\n",
            "Epoch 1709/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0446 - mse: 0.0446\n",
            "Epoch 1709: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0446 - mse: 0.0446 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1710/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1710: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1711/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1711: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0424 - val_mse: 0.0424 - lr: 1.0000e-04\n",
            "Epoch 1712/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1712: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1713/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1713: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 976ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1714/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1714: val_loss did not improve from 0.03273\n",
            "1/1 [==============================] - 1s 897ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0362 - val_mse: 0.0362 - lr: 1.0000e-04\n",
            "Epoch 1715/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0431 - mse: 0.0431\n",
            "Epoch 1715: val_loss improved from 0.03273 to 0.03269, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.0327 - val_mse: 0.0327 - lr: 1.0000e-04\n",
            "Epoch 1716/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1716: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1717/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1717: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1718/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1718: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1719/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1719: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1720/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0366 - mse: 0.0366\n",
            "Epoch 1720: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0366 - mse: 0.0366 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1721/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1721: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 1722/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1722: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 1723/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 1723: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 931ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1724/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1724: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1725/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mse: 0.0409\n",
            "Epoch 1725: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0409 - mse: 0.0409 - val_loss: 0.0403 - val_mse: 0.0403 - lr: 1.0000e-04\n",
            "Epoch 1726/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1726: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0394 - val_mse: 0.0394 - lr: 1.0000e-04\n",
            "Epoch 1727/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0336 - mse: 0.0336\n",
            "Epoch 1727: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.0398 - val_mse: 0.0398 - lr: 1.0000e-04\n",
            "Epoch 1728/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1728: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1729/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1729: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1730/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 1730: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1731/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 1731: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0327 - val_mse: 0.0327 - lr: 1.0000e-04\n",
            "Epoch 1732/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1732: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1733/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1733: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1734/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0347 - mse: 0.0347\n",
            "Epoch 1734: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1735/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 1735: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0410 - val_mse: 0.0410 - lr: 1.0000e-04\n",
            "Epoch 1736/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1736: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1737/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1737: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1738/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1738: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0419 - val_mse: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 1739/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1739: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0407 - val_mse: 0.0407 - lr: 1.0000e-04\n",
            "Epoch 1740/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1740: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 1741/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0445 - mse: 0.0445\n",
            "Epoch 1741: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 845ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1742/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mse: 0.0378\n",
            "Epoch 1742: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0378 - mse: 0.0378 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1743/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0362 - mse: 0.0362\n",
            "Epoch 1743: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1744/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 1744: val_loss did not improve from 0.03269\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0384 - val_mse: 0.0384 - lr: 1.0000e-04\n",
            "Epoch 1745/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1745: val_loss improved from 0.03269 to 0.03266, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0327 - val_mse: 0.0327 - lr: 1.0000e-04\n",
            "Epoch 1746/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1746: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0341 - val_mse: 0.0341 - lr: 1.0000e-04\n",
            "Epoch 1747/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1747: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 845ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1748/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0329 - mse: 0.0329\n",
            "Epoch 1748: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 906ms/step - loss: 0.0329 - mse: 0.0329 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 1749/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 1749: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1750/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1750: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1751/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1751: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0429 - val_mse: 0.0429 - lr: 1.0000e-04\n",
            "Epoch 1752/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1752: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1753/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mse: 0.0408\n",
            "Epoch 1753: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1754/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1754: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1755/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0445 - mse: 0.0445\n",
            "Epoch 1755: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 889ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1756/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 1756: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1757/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1757: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 929ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1758/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1758: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 940ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1759/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1759: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 880ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0354 - val_mse: 0.0354 - lr: 1.0000e-04\n",
            "Epoch 1760/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1760: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0345 - val_mse: 0.0345 - lr: 1.0000e-04\n",
            "Epoch 1761/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1761: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 891ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1762/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1762: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1763/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1763: val_loss did not improve from 0.03266\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1764/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1764: val_loss improved from 0.03266 to 0.03264, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0326 - val_mse: 0.0326 - lr: 1.0000e-04\n",
            "Epoch 1765/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1765: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1766/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 1766: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1767/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1767: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1768/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1768: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1769/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1769: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 902ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1770/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1770: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1771/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0445 - mse: 0.0445\n",
            "Epoch 1771: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0403 - val_mse: 0.0403 - lr: 1.0000e-04\n",
            "Epoch 1772/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 1772: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0371 - val_mse: 0.0371 - lr: 1.0000e-04\n",
            "Epoch 1773/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1773: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0418 - val_mse: 0.0418 - lr: 1.0000e-04\n",
            "Epoch 1774/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1774: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 898ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1775/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1775: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1776/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 1776: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 836ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0374 - val_mse: 0.0374 - lr: 1.0000e-04\n",
            "Epoch 1777/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1777: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1778/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0347 - mse: 0.0347\n",
            "Epoch 1778: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.0345 - val_mse: 0.0345 - lr: 1.0000e-04\n",
            "Epoch 1779/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1779: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0419 - val_mse: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 1780/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mse: 0.0408\n",
            "Epoch 1780: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0408 - mse: 0.0408 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1781/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1781: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1782/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0404 - mse: 0.0404\n",
            "Epoch 1782: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0404 - mse: 0.0404 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1783/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1783: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 839ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0361 - val_mse: 0.0361 - lr: 1.0000e-04\n",
            "Epoch 1784/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1784: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1785/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0445 - mse: 0.0445\n",
            "Epoch 1785: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1786/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 1786: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0418 - val_mse: 0.0418 - lr: 1.0000e-04\n",
            "Epoch 1787/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1787: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 901ms/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0399 - val_mse: 0.0399 - lr: 1.0000e-04\n",
            "Epoch 1788/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0328 - mse: 0.0328\n",
            "Epoch 1788: val_loss did not improve from 0.03264\n",
            "1/1 [==============================] - 1s 949ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1789/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0328 - mse: 0.0328\n",
            "Epoch 1789: val_loss improved from 0.03264 to 0.03263, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0326 - val_mse: 0.0326 - lr: 1.0000e-04\n",
            "Epoch 1790/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 1790: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1791/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 1791: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 885ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 1792/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mse: 0.0365\n",
            "Epoch 1792: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.0426 - val_mse: 0.0426 - lr: 1.0000e-04\n",
            "Epoch 1793/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1793: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0374 - val_mse: 0.0374 - lr: 1.0000e-04\n",
            "Epoch 1794/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1794: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0354 - val_mse: 0.0354 - lr: 1.0000e-04\n",
            "Epoch 1795/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1795: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 973ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1796/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1796: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0333 - val_mse: 0.0333 - lr: 1.0000e-04\n",
            "Epoch 1797/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1797: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 839ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0357 - val_mse: 0.0357 - lr: 1.0000e-04\n",
            "Epoch 1798/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0328 - mse: 0.0328\n",
            "Epoch 1798: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1799/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1799: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1800/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1800: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0449 - val_mse: 0.0449 - lr: 1.0000e-04\n",
            "Epoch 1801/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1801: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0340 - val_mse: 0.0340 - lr: 1.0000e-04\n",
            "Epoch 1802/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1802: val_loss did not improve from 0.03263\n",
            "1/1 [==============================] - 1s 852ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1803/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0427 - mse: 0.0427\n",
            "Epoch 1803: val_loss improved from 0.03263 to 0.03260, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0427 - mse: 0.0427 - val_loss: 0.0326 - val_mse: 0.0326 - lr: 1.0000e-04\n",
            "Epoch 1804/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1804: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0365 - val_mse: 0.0365 - lr: 1.0000e-04\n",
            "Epoch 1805/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 1805: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0449 - val_mse: 0.0449 - lr: 1.0000e-04\n",
            "Epoch 1806/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mse: 0.0376\n",
            "Epoch 1806: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1807/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0445 - mse: 0.0445\n",
            "Epoch 1807: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1808/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mse: 0.0381\n",
            "Epoch 1808: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0381 - mse: 0.0381 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1809/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1809: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1810/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1810: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1811/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1811: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0340 - val_mse: 0.0340 - lr: 1.0000e-04\n",
            "Epoch 1812/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1812: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1813/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1813: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0340 - val_mse: 0.0340 - lr: 1.0000e-04\n",
            "Epoch 1814/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1814: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 859ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0449 - val_mse: 0.0449 - lr: 1.0000e-04\n",
            "Epoch 1815/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mse: 0.0370\n",
            "Epoch 1815: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1816/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 1816: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1817/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1817: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1818/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0418 - mse: 0.0418\n",
            "Epoch 1818: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.0435 - val_mse: 0.0435 - lr: 1.0000e-04\n",
            "Epoch 1819/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mse: 0.0335\n",
            "Epoch 1819: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 901ms/step - loss: 0.0335 - mse: 0.0335 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1820/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mse: 0.0430\n",
            "Epoch 1820: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0430 - mse: 0.0430 - val_loss: 0.0424 - val_mse: 0.0424 - lr: 1.0000e-04\n",
            "Epoch 1821/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1821: val_loss did not improve from 0.03260\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1822/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1822: val_loss improved from 0.03260 to 0.03258, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0326 - val_mse: 0.0326 - lr: 1.0000e-04\n",
            "Epoch 1823/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1823: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0422 - val_mse: 0.0422 - lr: 1.0000e-04\n",
            "Epoch 1824/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1824: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 900ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0449 - val_mse: 0.0449 - lr: 1.0000e-04\n",
            "Epoch 1825/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mse: 0.0361\n",
            "Epoch 1825: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1826/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1826: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1827/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 1827: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1828/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1828: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0340 - val_mse: 0.0340 - lr: 1.0000e-04\n",
            "Epoch 1829/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1829: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0344 - val_mse: 0.0344 - lr: 1.0000e-04\n",
            "Epoch 1830/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1830: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0402 - val_mse: 0.0402 - lr: 1.0000e-04\n",
            "Epoch 1831/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mse: 0.0391\n",
            "Epoch 1831: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.0370 - val_mse: 0.0370 - lr: 1.0000e-04\n",
            "Epoch 1832/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1832: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 867ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0326 - val_mse: 0.0326 - lr: 1.0000e-04\n",
            "Epoch 1833/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0444 - mse: 0.0444\n",
            "Epoch 1833: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1834/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1834: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0418 - val_mse: 0.0418 - lr: 1.0000e-04\n",
            "Epoch 1835/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 1835: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0340 - val_mse: 0.0340 - lr: 1.0000e-04\n",
            "Epoch 1836/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0346 - mse: 0.0346\n",
            "Epoch 1836: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0418 - val_mse: 0.0418 - lr: 1.0000e-04\n",
            "Epoch 1837/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1837: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 866ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0344 - val_mse: 0.0344 - lr: 1.0000e-04\n",
            "Epoch 1838/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1838: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0393 - val_mse: 0.0393 - lr: 1.0000e-04\n",
            "Epoch 1839/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1839: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1840/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417\n",
            "Epoch 1840: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 850ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.0344 - val_mse: 0.0344 - lr: 1.0000e-04\n",
            "Epoch 1841/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1841: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1842/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 1842: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1843/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1843: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1844/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1844: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0332 - val_mse: 0.0332 - lr: 1.0000e-04\n",
            "Epoch 1845/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 1845: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0383 - val_mse: 0.0383 - lr: 1.0000e-04\n",
            "Epoch 1846/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1846: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 842ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1847/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1847: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 996ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1848/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1848: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1849/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1849: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1850/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mse: 0.0403\n",
            "Epoch 1850: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0403 - mse: 0.0403 - val_loss: 0.0344 - val_mse: 0.0344 - lr: 1.0000e-04\n",
            "Epoch 1851/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1851: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 1852/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1852: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1853/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 1853: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 1854/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1854: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1855/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mse: 0.0397\n",
            "Epoch 1855: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0397 - mse: 0.0397 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1856/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1856: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1857/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1857: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1858/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1858: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1859/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0353 - mse: 0.0353\n",
            "Epoch 1859: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 873ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1860/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 1860: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1861/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1861: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1862/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1862: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0428 - val_mse: 0.0428 - lr: 1.0000e-04\n",
            "Epoch 1863/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 1863: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1864/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1864: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 971ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1865/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1865: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1866/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 1866: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 848ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0357 - val_mse: 0.0357 - lr: 1.0000e-04\n",
            "Epoch 1867/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 1867: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 883ms/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1868/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1868: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 907ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0409 - val_mse: 0.0409 - lr: 1.0000e-04\n",
            "Epoch 1869/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390\n",
            "Epoch 1869: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 911ms/step - loss: 0.0390 - mse: 0.0390 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1870/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1870: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 884ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0419 - val_mse: 0.0419 - lr: 1.0000e-04\n",
            "Epoch 1871/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1871: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1872/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1872: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0406 - val_mse: 0.0406 - lr: 1.0000e-04\n",
            "Epoch 1873/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1873: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 877ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1874/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0328 - mse: 0.0328\n",
            "Epoch 1874: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.0382 - val_mse: 0.0382 - lr: 1.0000e-04\n",
            "Epoch 1875/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 1875: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1876/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 1876: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0448 - val_mse: 0.0448 - lr: 1.0000e-04\n",
            "Epoch 1877/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1877: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0434 - val_mse: 0.0434 - lr: 1.0000e-04\n",
            "Epoch 1878/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0360 - mse: 0.0360\n",
            "Epoch 1878: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0360 - mse: 0.0360 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1879/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1879: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1880/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0353 - mse: 0.0353\n",
            "Epoch 1880: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1881/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mse: 0.0407\n",
            "Epoch 1881: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1882/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1882: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 894ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1883/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 1883: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1884/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1884: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1885/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1885: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1886/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1886: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1887/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 1887: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1888/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1888: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 974ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0448 - val_mse: 0.0448 - lr: 1.0000e-04\n",
            "Epoch 1889/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0426 - mse: 0.0426\n",
            "Epoch 1889: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0426 - mse: 0.0426 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1890/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 1890: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1891/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0346 - mse: 0.0346\n",
            "Epoch 1891: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.0387 - val_mse: 0.0387 - lr: 1.0000e-04\n",
            "Epoch 1892/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1892: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1893/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1893: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0391 - val_mse: 0.0391 - lr: 1.0000e-04\n",
            "Epoch 1894/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364\n",
            "Epoch 1894: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0364 - mse: 0.0364 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1895/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1895: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1896/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1896: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1897/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1897: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1898/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1898: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0418 - val_mse: 0.0418 - lr: 1.0000e-04\n",
            "Epoch 1899/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mse: 0.0372\n",
            "Epoch 1899: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0372 - mse: 0.0372 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1900/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1900: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 842ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1901/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1901: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1902/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1902: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1903/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0327 - mse: 0.0327\n",
            "Epoch 1903: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0356 - val_mse: 0.0356 - lr: 1.0000e-04\n",
            "Epoch 1904/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1904: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1905/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1905: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1906/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mse: 0.0422\n",
            "Epoch 1906: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 848ms/step - loss: 0.0422 - mse: 0.0422 - val_loss: 0.0353 - val_mse: 0.0353 - lr: 1.0000e-04\n",
            "Epoch 1907/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mse: 0.0402\n",
            "Epoch 1907: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0402 - mse: 0.0402 - val_loss: 0.0359 - val_mse: 0.0359 - lr: 1.0000e-04\n",
            "Epoch 1908/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1908: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0372 - val_mse: 0.0372 - lr: 1.0000e-04\n",
            "Epoch 1909/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1909: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 876ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1910/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1910: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 843ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0392 - val_mse: 0.0392 - lr: 1.0000e-04\n",
            "Epoch 1911/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1911: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1912/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mse: 0.0399\n",
            "Epoch 1912: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 918ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1913/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mse: 0.0382\n",
            "Epoch 1913: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1914/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mse: 0.0377\n",
            "Epoch 1914: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0377 - mse: 0.0377 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1915/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1915: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 851ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1916/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 1916: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0373 - val_mse: 0.0373 - lr: 1.0000e-04\n",
            "Epoch 1917/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mse: 0.0406\n",
            "Epoch 1917: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0406 - mse: 0.0406 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1918/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1918: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1919/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 1919: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1920/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1920: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0416 - val_mse: 0.0416 - lr: 1.0000e-04\n",
            "Epoch 1921/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0327 - mse: 0.0327\n",
            "Epoch 1921: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 861ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0353 - val_mse: 0.0353 - lr: 1.0000e-04\n",
            "Epoch 1922/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1922: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0448 - val_mse: 0.0448 - lr: 1.0000e-04\n",
            "Epoch 1923/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 1923: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 858ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0448 - val_mse: 0.0448 - lr: 1.0000e-04\n",
            "Epoch 1924/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 1924: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 855ms/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1925/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1925: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1926/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1926: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 832ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0360 - val_mse: 0.0360 - lr: 1.0000e-04\n",
            "Epoch 1927/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 1927: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0379 - val_mse: 0.0379 - lr: 1.0000e-04\n",
            "Epoch 1928/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1928: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 853ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0381 - val_mse: 0.0381 - lr: 1.0000e-04\n",
            "Epoch 1929/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mse: 0.0383\n",
            "Epoch 1929: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 0.0383 - mse: 0.0383 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1930/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1930: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0356 - val_mse: 0.0356 - lr: 1.0000e-04\n",
            "Epoch 1931/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 1931: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1932/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1932: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1933/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 1933: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 993ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1934/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0334 - mse: 0.0334\n",
            "Epoch 1934: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0334 - mse: 0.0334 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1935/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mse: 0.0374\n",
            "Epoch 1935: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0374 - mse: 0.0374 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1936/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 1936: val_loss did not improve from 0.03258\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1937/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1937: val_loss improved from 0.03258 to 0.03250, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0325 - val_mse: 0.0325 - lr: 1.0000e-04\n",
            "Epoch 1938/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mse: 0.0369\n",
            "Epoch 1938: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0369 - mse: 0.0369 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 1939/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0425 - mse: 0.0425\n",
            "Epoch 1939: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0425 - mse: 0.0425 - val_loss: 0.0389 - val_mse: 0.0389 - lr: 1.0000e-04\n",
            "Epoch 1940/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1940: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 848ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0367 - val_mse: 0.0367 - lr: 1.0000e-04\n",
            "Epoch 1941/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mse: 0.0392\n",
            "Epoch 1941: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0392 - mse: 0.0392 - val_loss: 0.0380 - val_mse: 0.0380 - lr: 1.0000e-04\n",
            "Epoch 1942/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1942: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 865ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0357 - val_mse: 0.0357 - lr: 1.0000e-04\n",
            "Epoch 1943/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 1943: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1944/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1944: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0357 - val_mse: 0.0357 - lr: 1.0000e-04\n",
            "Epoch 1945/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mse: 0.0389\n",
            "Epoch 1945: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1946/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 1946: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0352 - val_mse: 0.0352 - lr: 1.0000e-04\n",
            "Epoch 1947/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0339 - mse: 0.0339\n",
            "Epoch 1947: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 911ms/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n",
            "Epoch 1948/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mse: 0.0400\n",
            "Epoch 1948: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 860ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1949/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1949: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 892ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0356 - val_mse: 0.0356 - lr: 1.0000e-04\n",
            "Epoch 1950/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mse: 0.0368\n",
            "Epoch 1950: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 845ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.0352 - val_mse: 0.0352 - lr: 1.0000e-04\n",
            "Epoch 1951/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mse: 0.0356\n",
            "Epoch 1951: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 896ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.0343 - val_mse: 0.0343 - lr: 1.0000e-04\n",
            "Epoch 1952/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 1952: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 899ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0425 - val_mse: 0.0425 - lr: 1.0000e-04\n",
            "Epoch 1953/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mse: 0.0367\n",
            "Epoch 1953: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0367 - mse: 0.0367 - val_loss: 0.0421 - val_mse: 0.0421 - lr: 1.0000e-04\n",
            "Epoch 1954/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mse: 0.0416\n",
            "Epoch 1954: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0416 - mse: 0.0416 - val_loss: 0.0377 - val_mse: 0.0377 - lr: 1.0000e-04\n",
            "Epoch 1955/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mse: 0.0358\n",
            "Epoch 1955: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 893ms/step - loss: 0.0358 - mse: 0.0358 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1956/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1956: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 847ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0423 - val_mse: 0.0423 - lr: 1.0000e-04\n",
            "Epoch 1957/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mse: 0.0395\n",
            "Epoch 1957: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 854ms/step - loss: 0.0395 - mse: 0.0395 - val_loss: 0.0401 - val_mse: 0.0401 - lr: 1.0000e-04\n",
            "Epoch 1958/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 1958: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0417 - val_mse: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 1959/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352\n",
            "Epoch 1959: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0352 - mse: 0.0352 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1960/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mse: 0.0428\n",
            "Epoch 1960: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0428 - mse: 0.0428 - val_loss: 0.0352 - val_mse: 0.0352 - lr: 1.0000e-04\n",
            "Epoch 1961/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375\n",
            "Epoch 1961: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 857ms/step - loss: 0.0375 - mse: 0.0375 - val_loss: 0.0375 - val_mse: 0.0375 - lr: 1.0000e-04\n",
            "Epoch 1962/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1962: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 875ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0396 - val_mse: 0.0396 - lr: 1.0000e-04\n",
            "Epoch 1963/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0326 - mse: 0.0326\n",
            "Epoch 1963: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1964/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1964: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1965/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0386 - mse: 0.0386\n",
            "Epoch 1965: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 862ms/step - loss: 0.0386 - mse: 0.0386 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1966/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1966: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 881ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1967/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 1967: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 849ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0420 - val_mse: 0.0420 - lr: 1.0000e-04\n",
            "Epoch 1968/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1968: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0427 - val_mse: 0.0427 - lr: 1.0000e-04\n",
            "Epoch 1969/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1969: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 871ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0390 - val_mse: 0.0390 - lr: 1.0000e-04\n",
            "Epoch 1970/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1970: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 869ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1971/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0415 - mse: 0.0415\n",
            "Epoch 1971: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.0415 - mse: 0.0415 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1972/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0339 - mse: 0.0339\n",
            "Epoch 1972: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0339 - mse: 0.0339 - val_loss: 0.0374 - val_mse: 0.0374 - lr: 1.0000e-04\n",
            "Epoch 1973/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1973: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0378 - val_mse: 0.0378 - lr: 1.0000e-04\n",
            "Epoch 1974/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1974: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0357 - val_mse: 0.0357 - lr: 1.0000e-04\n",
            "Epoch 1975/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384\n",
            "Epoch 1975: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 954ms/step - loss: 0.0384 - mse: 0.0384 - val_loss: 0.0385 - val_mse: 0.0385 - lr: 1.0000e-04\n",
            "Epoch 1976/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mse: 0.0396\n",
            "Epoch 1976: val_loss did not improve from 0.03250\n",
            "1/1 [==============================] - 1s 951ms/step - loss: 0.0396 - mse: 0.0396 - val_loss: 0.0355 - val_mse: 0.0355 - lr: 1.0000e-04\n",
            "Epoch 1977/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 1977: val_loss improved from 0.03250 to 0.03244, saving model to /content/drive/My Drive/Tese/models/ConvLSTM2D/ConvLSTM2D.h5\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0324 - val_mse: 0.0324 - lr: 1.0000e-04\n",
            "Epoch 1978/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1978: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 886ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1979/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1979: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1980/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1980: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 872ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0368 - val_mse: 0.0368 - lr: 1.0000e-04\n",
            "Epoch 1981/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mse: 0.0385\n",
            "Epoch 1981: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 836ms/step - loss: 0.0385 - mse: 0.0385 - val_loss: 0.0395 - val_mse: 0.0395 - lr: 1.0000e-04\n",
            "Epoch 1982/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mse: 0.0423\n",
            "Epoch 1982: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 878ms/step - loss: 0.0423 - mse: 0.0423 - val_loss: 0.0424 - val_mse: 0.0424 - lr: 1.0000e-04\n",
            "Epoch 1983/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mse: 0.0388\n",
            "Epoch 1983: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 879ms/step - loss: 0.0388 - mse: 0.0388 - val_loss: 0.0405 - val_mse: 0.0405 - lr: 1.0000e-04\n",
            "Epoch 1984/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0363 - mse: 0.0363\n",
            "Epoch 1984: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 882ms/step - loss: 0.0363 - mse: 0.0363 - val_loss: 0.0400 - val_mse: 0.0400 - lr: 1.0000e-04\n",
            "Epoch 1985/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0373 - mse: 0.0373\n",
            "Epoch 1985: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0373 - mse: 0.0373 - val_loss: 0.0363 - val_mse: 0.0363 - lr: 1.0000e-04\n",
            "Epoch 1986/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mse: 0.0380\n",
            "Epoch 1986: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0380 - mse: 0.0380 - val_loss: 0.0447 - val_mse: 0.0447 - lr: 1.0000e-04\n",
            "Epoch 1987/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mse: 0.0357\n",
            "Epoch 1987: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0357 - mse: 0.0357 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1988/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371\n",
            "Epoch 1988: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 929ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.0397 - val_mse: 0.0397 - lr: 1.0000e-04\n",
            "Epoch 1989/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0419 - mse: 0.0419\n",
            "Epoch 1989: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.0355 - val_mse: 0.0355 - lr: 1.0000e-04\n",
            "Epoch 1990/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mse: 0.0387\n",
            "Epoch 1990: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 945ms/step - loss: 0.0387 - mse: 0.0387 - val_loss: 0.0338 - val_mse: 0.0338 - lr: 1.0000e-04\n",
            "Epoch 1991/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1991: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0369 - val_mse: 0.0369 - lr: 1.0000e-04\n",
            "Epoch 1992/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0442 - mse: 0.0442\n",
            "Epoch 1992: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 841ms/step - loss: 0.0442 - mse: 0.0442 - val_loss: 0.0386 - val_mse: 0.0386 - lr: 1.0000e-04\n",
            "Epoch 1993/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mse: 0.0424\n",
            "Epoch 1993: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 863ms/step - loss: 0.0424 - mse: 0.0424 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1994/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mse: 0.0398\n",
            "Epoch 1994: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 843ms/step - loss: 0.0398 - mse: 0.0398 - val_loss: 0.0376 - val_mse: 0.0376 - lr: 1.0000e-04\n",
            "Epoch 1995/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mse: 0.0359\n",
            "Epoch 1995: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 856ms/step - loss: 0.0359 - mse: 0.0359 - val_loss: 0.0388 - val_mse: 0.0388 - lr: 1.0000e-04\n",
            "Epoch 1996/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mse: 0.0401\n",
            "Epoch 1996: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 870ms/step - loss: 0.0401 - mse: 0.0401 - val_loss: 0.0399 - val_mse: 0.0399 - lr: 1.0000e-04\n",
            "Epoch 1997/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0327 - mse: 0.0327\n",
            "Epoch 1997: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 868ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0403 - val_mse: 0.0403 - lr: 1.0000e-04\n",
            "Epoch 1998/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mse: 0.0420\n",
            "Epoch 1998: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 874ms/step - loss: 0.0420 - mse: 0.0420 - val_loss: 0.0331 - val_mse: 0.0331 - lr: 1.0000e-04\n",
            "Epoch 1999/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mse: 0.0421\n",
            "Epoch 1999: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.0424 - val_mse: 0.0424 - lr: 1.0000e-04\n",
            "Epoch 2000/2000\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.0355 - mse: 0.0355\n",
            "Epoch 2000: val_loss did not improve from 0.03244\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0355 - mse: 0.0355 - val_loss: 0.0358 - val_mse: 0.0358 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test results\n",
        "\n",
        "tf.data.experimental.enable_debug_mode()\n",
        "batch1 = input.get_single_element()\n",
        "loaded_model = tf.keras.models.load_model(base_path + 'models/ConvLSTM2D/ConvLSTM2D.h5')\n",
        "prediction = loaded_model.predict(batch1[0], batch_size=BATCH_SIZE)\n",
        "prediction.shape\n",
        "\n",
        "#Concatenate images for side-by-side viewing\n",
        "sidebyside = batch1[0][0][0] #set = to first image\n",
        "for j in range(1, len(batch1[0][0])):\n",
        "    sidebyside = np.concatenate((sidebyside, batch1[0][0][j]), axis=1)\n",
        "plt.imshow(cv.cvtColor(sidebyside, cv.COLOR_BGR2RGB))\n",
        "plt.title(\"Input Sequence\")\n",
        "plt.show()\n",
        "\n",
        "#Plot Target Frame\n",
        "target_image = batch1[1][0].numpy()\n",
        "plt.imshow(cv.cvtColor(target_image, cv.COLOR_BGR2RGB))\n",
        "plt.title(\"Target Image\")\n",
        "plt.show()\n",
        "\n",
        "#Plot Output Frame\n",
        "#prediction_image = prediction[0]\n",
        "plt.imshow(cv.cvtColor(prediction[0], cv.COLOR_BGR2RGB))\n",
        "plt.title(\"Prediction\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "fxyXge2KapUp",
        "outputId": "12eb51e3-ddd6-4497-a0d3-76049a0d590e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 9s 9s/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}